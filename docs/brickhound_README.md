# Permissions Analysis Tool

**Six Degrees of Databricks Admin**

Permissions Analysis Tool is a security analysis tool for Databricks environments. It maps privilege relationships across Databricks objects to identify attack paths and privilege escalation opportunities.

**ğŸš€ Runs entirely on Databricks** - No external infrastructure needed. See [DATABRICKS_SETUP.md](DATABRICKS_SETUP.md) for setup.

## Overview

Permissions Analysis Tool leverages graph theory to reveal hidden relationships within Databricks identity and access management systems. Using GraphFrames on Databricks Spark, it analyzes complex privilege relationships across:

- **Users & Groups**: Account and workspace-level identities
- **Workspaces**: Databricks workspace configurations
- **Clusters**: Interactive and job clusters
- **Notebooks**: Code assets and their permissions
- **Jobs**: Scheduled workflows
- **SQL Warehouses**: Query engines
- **Unity Catalog Objects**: Catalogs, schemas, tables, volumes, functions
- **Secret Scopes**: Credential management
- **Repos**: Git integrations
- **Instance Pools**: Cluster resource pools

## Architecture

**Permissions Analysis Tool runs entirely on Databricks** - no external infrastructure needed.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DATABRICKS WORKSPACE                       â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     Permissions Analysis Tool Collector Notebook   â”‚   â”‚
â”‚  â”‚   (Databricks SDK + REST API)                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚                                   â”‚
â”‚                     â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     Delta Lake Graph Storage                   â”‚   â”‚
â”‚  â”‚   Catalog: brickhound                          â”‚   â”‚
â”‚  â”‚   Schema: graph                                â”‚   â”‚
â”‚  â”‚     - vertices (nodes)                         â”‚   â”‚
â”‚  â”‚     - edges (relationships)                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚                                   â”‚
â”‚                     â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     GraphFrames Analysis Engine                â”‚   â”‚
â”‚  â”‚   (Spark + GraphFrames on Databricks)          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚                                   â”‚
â”‚                     â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     Visualization & Dashboards                 â”‚   â”‚
â”‚  â”‚   (Plotly + NetworkX in Notebooks)             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Benefits**:
- âœ… **No external dependencies** - Everything runs on Databricks
- âœ… **Data never leaves your workspace** - Stored in Delta Lake
- âœ… **Native authentication** - Uses workspace context automatically
- âœ… **Scalable** - Leverage Spark for large-scale analysis
- âœ… **Integrated** - Results viewable in Databricks SQL, notebooks, and dashboards

## Features

### ğŸ¯ Interactive Web App (NEW!)
- âœ¨ **User-friendly web interface** for permission analysis
- ğŸ” **9 interactive features** across two categories:
  - **Analysis Tools**: Principal Analysis, Resource Analysis, Escalation Paths, Impersonation Paths
  - **Security Reports**: High Privilege, Isolated, Orphaned, Over-Privileged, Secret Scopes
- ğŸ“Š **Real-time results** - No code required
- ğŸš€ **Deploy as Databricks App** - One-click deployment
- ğŸ“± **Share with security team** - Secure web access

**â†’ See [app/README.md](app/README.md) for deployment guide**

### Data Collection
- âœ… Enumerate all Databricks objects via REST API
- âœ… Extract permission relationships
- âœ… Map group memberships and role assignments
- âœ… Capture Unity Catalog grants
- âœ… Support for both Account-level and Workspace-level APIs

### Graph Analysis
- ğŸ” Find privilege escalation paths
- ğŸ” Identify over-privileged users
- ğŸ” Discover orphaned resources
- ğŸ” Map blast radius of compromised identities
- ğŸ” Detect permission inheritance chains

### Visualization
- ğŸ“Š Interactive graph visualizations
- ğŸ“Š Path highlighting
- ğŸ“Š Risk scoring
- ğŸ“Š Export capabilities

## Quick Start

### Prerequisites
- **Databricks Workspace** (AWS, Azure, or GCP)
- **DBR 13.3 LTS or higher** (for Unity Catalog support)
- **Permissions**: Workspace Admin (minimum) or Metastore Admin (recommended)  
  See [PERMISSIONS.md](PERMISSIONS.md) for detailed permission requirements
- **Cluster** with access mode: Shared or Single User

### Installation (On Databricks)

BrickHound runs entirely within your Databricks workspace. No external servers or local setup required.

#### Step 1: Clone or Upload Notebooks

**Option A: Using Repos (Recommended)**
1. In Databricks workspace, go to **Repos**
2. Click **Add Repo**
3. Enter URL: `https://github.com/arunpamulapati/BrickHound`
4. All notebooks and code will be available in your workspace

**Option B: Manual Upload**
1. Download this repository
2. Import the `notebooks/` folder into your Databricks workspace

#### Step 2: Create a Cluster (or use existing)

Create or use any cluster with:
- **DBR**: 13.3 LTS or higher
- **Access Mode**: Shared or Single User
- **Libraries**: âœ… Installed automatically by notebooks!

#### Step 3: Run Collection Notebook

1. Open `notebooks/permission_analysis_data_collection.py`
2. Attach to your cluster
3. Run all cells - libraries install automatically!
4. The notebook uses your workspace context (no tokens needed!)
5. Data is saved to Delta Lake tables in your workspace

**First run**: Takes a few extra minutes to install libraries. Subsequent runs are faster.

#### Step 4: Analyze and Visualize

Run the analysis notebooks:
- `01_principal_resource_analysis.py` - Principal and resource permission queries
- `02_escalation_paths.py` - Privilege escalation analysis
- `03_impersonation_analysis.py` - Impersonation path finding
- `04_advanced_reports.py` - Comprehensive security reports

## Project Structure

```
Permissions Analysis Tool/
â”œâ”€â”€ app/                           # ğŸ¯ WEB APP - Interactive Gradio UI
â”‚   â”œâ”€â”€ app.py                     # Main Gradio application (7 analyses)
â”‚   â”œâ”€â”€ app.yaml                   # Databricks App configuration
â”‚   â”œâ”€â”€ requirements.txt           # App dependencies
â”‚   â””â”€â”€ README.md                  # App deployment guide
â”‚
â”œâ”€â”€ notebooks/                      # ğŸ‘ˆ START HERE - Databricks Notebooks
â”‚   â”œâ”€â”€ 00_config.py               # âš™ï¸  Configuration (catalog/schema settings)
â”‚   â”œâ”€â”€ 00_analysis_common.py      # ğŸ“‹ Shared analysis setup and utilities
â”‚   â”œâ”€â”€ permission_analysis_data_collection.py      # ğŸ“Š Data collection via Databricks SDK
â”‚   â”œâ”€â”€ 01_principal_resource_analysis.py  # ğŸ” Principal and resource queries
â”‚   â”œâ”€â”€ 02_escalation_paths.py     # âš¡ Privilege escalation path analysis
â”‚   â”œâ”€â”€ 03_impersonation_analysis.py      # ğŸ­ Impersonation path finding
â”‚   â””â”€â”€ 04_advanced_reports.py     # ğŸ“ˆ Security reports (High Privilege, Isolated, Orphaned, etc.)
â”‚
â”œâ”€â”€ brickhound/                    # Python library (auto-imported in notebooks)
â”‚   â”œâ”€â”€ collector/
â”‚   â”‚   â””â”€â”€ core.py               # Main collector orchestrator
â”‚   â”œâ”€â”€ graph/
â”‚   â”‚   â”œâ”€â”€ schema.py             # Delta Lake schema definitions
â”‚   â”‚   â”œâ”€â”€ builder.py            # Graph construction logic
â”‚   â”‚   â””â”€â”€ permissions_resolver.py # Recursive permission analysis engine
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ api_client.py         # Databricks SDK wrapper
â”‚       â””â”€â”€ config.py             # Configuration classes
â”‚
â”œâ”€â”€ requirements.txt              # For local development only
â”œâ”€â”€ setup.py                      # Package setup
â””â”€â”€ README.md                     # This file
```

**ğŸ‘‰ START HERE**: See [DATABRICKS_SETUP.md](DATABRICKS_SETUP.md) for complete setup instructions.

**Quick Start**:
1. Add repo to Databricks
2. Update `CATALOG` in `notebooks/00_config.py` (default: "main")
3. Run `notebooks/permission_analysis_data_collection.py` to collect data
4. **Option A:** Deploy `app/` as Databricks App (user-friendly web UI) - update `app.yaml` first
5. **Option B:** Run analysis notebooks directly (for deep dives)

## Databricks Objects Mapped

### Identity & Access
- Users (account & workspace)
- Groups (account & workspace)
- Service Principals
- Entitlements

### Compute Resources
- Clusters (all-purpose, job, SQL)
- Instance Pools
- Cluster Policies

### Data & Analytics
- Unity Catalog: Catalogs, Schemas, Tables, Views, Volumes, Functions
- Legacy Hive Metastore objects
- SQL Warehouses

### Development Assets
- Notebooks
- Repos
- Libraries
- MLflow Models & Experiments

### Orchestration
- Jobs
- Job runs
- Pipelines (Delta Live Tables)

### Security & Governance
- Secret Scopes
- Tokens
- IP Access Lists

## Important Notes

### Unity Catalog Requirement
Permissions Analysis Tool stores data in Delta Lake using Unity Catalog. You need:
- An existing Unity Catalog catalog (e.g., `main` or custom catalog)
- Permission to CREATE SCHEMA in that catalog
- The collection notebook will verify catalog exists and help you choose one

### Metadata Storage
Graph metadata (properties, permissions details) is stored as JSON strings in Delta Lake columns. This ensures compatibility with Spark DataFrame operations.

## Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## License

```
Copyright 2025 Arun Pamulapati

Licensed under the Apache License, Version 2.0
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
```

## Security & Permissions

Permissions Analysis Tool requires read permissions to collect data. See [PERMISSIONS.md](PERMISSIONS.md) for:
- Minimum required permissions
- Recommended service principal setup
- Permission scoping options
- Security best practices

**Summary**:
- **Minimum**: Workspace User (limited visibility)
- **Recommended**: Workspace Admin + Metastore Admin
- **Production**: Service Principal with OAuth (see setup guide)

## Acknowledgments

Built for the Databricks security community.

## Roadmap

- [ ] Initial data collector implementation
- [ ] Delta Lake graph schema
- [ ] GraphFrames integration
- [ ] Pre-built attack path queries
- [ ] Interactive visualizations
- [ ] CI/CD for Databricks deployment
- [ ] Multi-workspace support
- [ ] Integration with security tools (SIEM, etc.)

