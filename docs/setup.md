## Checklist to prepare for SAT setup

**Note**: SAT creates a new **security_analysis** databses and Delta tables. 

If you are an existing SAT user please run the following command to reset your Database in your DBSQL SQL Editor.

  ``` 
    drop  database security_analysis cascade
   ``` 
Please make sure you are using - in all secret key names as opposed to _ .   

**Note**: SAT is a productivity tool to help verify security configurations of Databricks deployments, its not meant to be used as certification or attestation of your deployments. SAT project is regulary updated to improve correctness of checks, add new checks, fix bugs. Please send your feedback and comments to sat@databricks.com.

You will need the following information to set up SAT, we will show you how to gather them in the next section.

We created a companion Security Analysis Tool (SAT) [Setup primer video](https://www.youtube.com/watch?v=kLSc3UHKL40) and a [Deployment checklist sheet](./SAT%20Deployment%20Checklist.xlsx) to help prepare for the SAT setup. 

 1. Databricks Account ID 
 2. Authentication information:
    * **AWS:** Administrative user id and password  (To call the account REST APIs)
    * **Azure:** Azure subscriptionId,  The Directory Tenant ID, The Application Client ID  and The Client secret generated by AAD   (To call the account REST APIs)
    * **GCP:** Service account key and impersonate-service-account  (To call the account REST APIs)
 3. A Single user job cluster (To run the SAT checks)
 4. Databricks SQL Warehouse  (To run the SQL dashboard)
 5. Ensure that Databricks Repos is enabled (To access the SAT git)
 6. Pipy access from your workspace (To install the SAT utility library)
 7. PAT token for the SAT primary deployment workspace 
  
## Prerequisites 

 (Estimated time to complete these steps: 15 - 30 mins)
<summary>Setup instructions</summary> 
Please gather the following information before you start setting up: 
 
 1. Databricks Account ID 
     * Please test your administrator account and password to make sure this is a working account: <br/>
       **AWS:** [https://accounts.cloud.databricks.com](https://accounts.cloud.databricks.com) <br/>
       **Azure:** [https://accounts.azuredatabricks.net](https://accounts.azuredatabricks.net) <br/>
       **GCP:** [https://accounts.gcp.databricks.com](https://accounts.gcp.databricks.com)<br/>
       
     * Copy the account id as shown below

        <img src="./images/account_id.png" width="30%" height="30%">

 2. A Single user cluster  
    *  Databricks Runtime Version  11.3 LTS or above
    *  Node type i3.xlarge (please start with a max of a two node cluster and adjust to 5 nodes if you have many workspaces that will be analyzed with SAT)  

        <img src="./images/job_cluster.png" width="50%" height="50%">
     **Note:**  In our tests we found that the full run of SAT takes about 10 mins per workspace. 
     
 3. Databricks SQL Warehouse  
    * Goto SQL (pane) -> SQL Warehouse -> and pick the SQL Warehouse for your dashboard and note down the ID as shown below
    * This Warehouse needs to be in a running state when you run steps in the Setup section.
    
        <img src="./images/dbsqlwarehouse_id.png" width="50%" height="50%">

 4. Databricks Repos to access SAT git
    Import git repo into Databricks repo 

    ``` 
           https://github.com/databricks-industry-solutions/security-analysis-tool
    ```


      <img src="./images/git_import.png" width="50%" height="50%">

 5. Please confirm that PyPI access is available

    * Open the \<SATProject\>/notebooks/Includes/install_sat_sdk  and run on the cluster that was created in the Step 2 above. 
    Please make sure there are no errors.
    If your deployment does not allow PyPI access please see the FAQ below at the end of this doc to see alternative options. 

6. Configure secrets

  * Download and setup Databricks CLI by following the instructions [here](https://docs.databricks.com/dev-tools/cli/index.html) on your work laptop or your virtual workstation.   
  * Note: if you have multiple Databricks profiles you will need to use --profile <profile name> switch to access the correct workspace,
    follow the instructions [here](https://docs.databricks.com/dev-tools/cli/index.html#connection-profiles) . Throughout the documentation below we use an example profile **e2-sat**, please adjust your commands as per your workspace profile or exclude  --profile <optional-profile-name> if you are using the default profile. 
  * Setup authentication to your Databricks workspace by following the instructions [here](https://docs.databricks.com/dev-tools/cli/index.html#set-up-authentication)

       ```
            databricks configure --token --profile e2-sat
       ```

     <img src="./images/cli_authentication.png" width="50%" height="50%">

     You should see a listing of folders in your workspace : 
      ```
           databricks --profile e2-sat workspace ls
      ```

     <img src="./images/workspace_ls.png" width="50%" height="50%">


  *  Set up the secret scope with the scope name you prefer and note it down:
     
     Note: The values you place below are case sensitive and need to be exact. 
 
     ```
      databricks --profile e2-sat  secrets create-scope --scope sat_scope
      ```

     For more details refer [here](https://docs.databricks.com/dev-tools/cli/secrets-cli.html) 

  * Authentication information:
  
    <details>
     <summary>AWS instructions</summary>  
     Create username secret and password secret of administrative user id and password  as  "user" and "pass" under the above "sat_scope" scope using Databricks Secrets CLI 

       *  Input your Databricks account console admin username to store it in a the secret store
           ```
           databricks --profile e2-sat secrets put --scope sat_scope --key user
           ```

       *  Input your Databricks account console admin account password to store it in a the secret store

           ```
           databricks --profile e2-sat secrets put --scope sat_scope --key pass
           ```    
    </details>
 
    <details>
     <summary>Azure instructions</summary>  

      We will be using the instructions in [Get Azure AD tokens for service principals](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/aad/service-prin-aad-token).
       *  Follow the document above and complete all steps in the "Provision a service principal in Azure portal" only as detailed in the document.
       *  On the application page’s Overview page, in the Essentials section, copy the following values: (You will need this in the step below)
          * Application (client) ID as client_id
          * Directory (tenant) ID tenant_id
          * client_secret (The secret generated by AAD during your confidential app registration)  client_credential
          <img src="./images/azure_sp_essentials.png" width="50%" height="50%">
       *  Notedown the "Display name" as Service Principle name. (You will need this in the step below)
       *  Notedown the Subscription ID as subscription_id from the Subscriptions section of the Azure portal
       *  Please add the service principle with "Reader" role in Access control (IAM) using Role assignments under your [subscription, Access control (IAM) section](https://learn.microsoft.com/en-us/azure/role-based-access-control/role-assignments-portal#step-2-open-the-add-role-assignment-page) 
           <img src="./images/azure_role_assignment.png" width="70%" height="70%"> 
    </details> 
 
    <details>
     <summary>GCP instructions</summary>  

      We will be using the instructions in [Authenticate to workspace or account APIs with a Google ID token](https://docs.gcp.databricks.com/dev-tools/api/latest/authentication-google-id-account-private-preview.html).
       *  Follow the document above and complete all steps in the [Step 1](https://docs.gcp.databricks.com/dev-tools/api/latest/authentication-google-id-account-private-preview.html#step-1-create-two-service-accounts) as detailed in the document.
       *  Notedown the name and location of service account key json file. (You will need this in the step below)
       *  Notedown the impersonate-service-account email address. (You will need this in the step below)
     
    </details>  
 
  * Create a secret for the workspace PAT token

      **Note**: Replace \<workspace_id\> with your SAT deployment workspace id. 
       You can find your workspace id by following the instructions [here](https://docs.databricks.com/workspace/workspace-details.html)

       You can create a PAT token by following the instructions [here](https://docs.databricks.com/dev-tools/api/latest/authentication.html#generate-a-personal-access-token). Please pay attention to _ and - , scopes use _ and keys must use - .


       ```
       databricks --profile e2-sat secrets put --scope sat_scope --key sat-token-<workspace_id> 
       ``` 

   * In your environment where you imported SAT project from git (Refer to Step 4 in Prerequisites) Open the \<SATProject\>/notebooks/Utils/initialize notebook and modify the JSON string with :  
     * Set the value for the account_id 
     * Set the value for the sql_warehouse_id
     * Set the value for username_for_alerts
     * databricks secrets scope/key names to pick the secrets from the steps above.

     * Your config in  \<SATProject\>/notebooks/Utils/initialize CMD 4 should look like this:

        ```
              {
                 "account_id":"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee",  <- update this value
                 "sql_warehouse_id":"4d9fef7de2b9995c",     <- update this value
                 "username_for_alerts":"john.doe@org.com", <- update this value with a valid Databricks user id
                 "verbosity":"info"
              }

        ```
     
     *  Azure additional configurations:
          <details>
           <summary>Azure instructions</summary>  
 
          * Setup the Client secret in a secret 
             ```
               databricks --profile e2-sat secrets put --scope sat_scope --key client-secret
             ``` 
          * Update config in  \<SATProject\>/notebooks/Utils/initialize CMD 7
              * Set the value for the subscription_id
              * Set the value for the tenant_id
              * Set the value for client_id
          * Your config in  \<SATProject\>/notebooks/Utils/initialize CMD 7 should look like this:

             ```
                   json_.update({
                      "account_id":"azure",
                      "subscription_id":"xxxxxxxx-fake-46d6-82bd-5cc8d962326b", # Azure subscriptionId
                      "tenant_id":"xxxxxxxx-fake-4280-9796-b1864a10effd", #The Directory (tenant) ID for the application registered in Azure AD.
                      "client_id":"xxxxxxxx-fake-4q1a-bb68-6ear3b26btbd", # The Application (client) ID for the application registered in Azure AD.
                      "client_secret_key":"client-secret",  #The secret generated by AAD during your confidential app registration
                      "use_mastercreds":True
                  })

             ``` 
           *  Follow the instructions "Add a service principal to a workspace" [Add a service principal to a workspace using the admin console](https://learn.microsoft.com/en-us/azure/databricks/administration-guide/users-groups/service-principals#--add-a-service-principal-to-a-workspace) as detailed in the document for each workspce you would like to analyze.

           <img src="./images/azure_service_account_workspaces_api.png" width="70%" height="70%">

          </details>                                                              
                                                              
     *  GCP additional configurations:
          <details>
           <summary>GCP instructions</summary>  
 
          * Upload the service account key json file, adjust \<key file name\> 
             ```
               databricks --profile e2-sat fs cp <key file name>  dbfs:/FileStore/tables/<key file name> --overwrite
             ``` 
          * Update config in  \<SATProject\>/notebooks/Utils/initialize CMD 4
          * Set the value for the service_account_key_file_path
          * Set the value for impersonate_service_account
          * Your config in  \<SATProject\>/notebooks/Utils/initialize CMD 6 should look like this:

             ```
                   #GCP configurations 
                      json_.update({
                         "service_account_key_file_path":"/dbfs/FileStore/tables/SA_1_key.json",    <- update this value
                         "impersonate_service_account":"xyz-sa-2@project.iam.gserviceaccount.com",  <- update this value
                         "use_mastercreds":False <- don't update this value                                  
                      })
             ``` 
           *  Follow the instructions in Step 4 of [Authenticate to workspace or account APIs with a Google ID token]([https://docs.gcp.databricks.com/dev-tools/api/latest/authentication-google-id-account-private-preview.html#step-1-create-two-service-accounts](https://docs.gcp.databricks.com/dev-tools/api/latest/authentication-google-id-account-private-preview.html#step-4-add-the-service-account-as-a-workspace-or-account-user)) as detailed in the document for each workspce you would like to analyze and the account to add your main service account (SA-2).

           <img src="./images/gcp_service_account_workspaces_api.png" width="70%" height="70%">

           <img src="./images/gcp_service_account_acounts_api.png" width="70%" height="70%">
          </details>
 
## Setup option 1 (Simple and recommended method)
                                                           
  (Estimated time to complete these steps: 15 - 30 mins, varies by number of workspaces in the account)  
 This method uses admin/service principle credentials (configured in the Step 6 of Prerequisites section) to call workspace APIs.   
                                                           
 Make sure both SAT job cluster (Refer to Prerequisites Step 2 ) and Warehouse (Refer to Prerequisites Step 3) are running.                                                                   
<details>
  <summary>Setup instructions</summary>                                                                          
 Following is the one time easy setup to get your workspaces setup with the SAT:

* Attach  \<SATProject\>/notebooks/security_analysis_initializer to the SAT cluster you created above and Run -> Run all 
 
    <img src="./images/initialize_sat.png" width="70%" height="70%">
 
 
    
    <img src="./images/initialize_sat_complete.png" width="70%" height="70%">
   
</details>
 
## Setup option 2 (Most flexible for the power users)
 
  (Estimated time to complete these steps: 30 mins)  
   This method uses admin credentials (configured in the Step 6 of Prerequisites section) by default to call workspace APIs. But can be changed to use workspace PAT tokens instead.
<details>
  <summary>Setup instructions</summary> 
 Following are the one time easy steps to get your workspaces setup with the SAT:
                  <img src="./images/setup_steps.png" width="100%" height="100%">  
 
1. List account workspaces to analyze with SAT
   * Goto  \<SATProject\>/notebooks/Setup/1.list_account_workspaces_to_conf_file and Run -> Run all 
   * This creates a configuration file as noted at the bottom of the notebook.

    <img src="./images/list_workspaces.png" width="70%" height="70%">
   
   
2. Generate secrets setup file (AWS only. Not recommended for Azure and GCP)
  Note: You can skip this step and go to step 4, if you would like to use admin credentials (configured in the Step 6 of Prerequisites section) to call workspace APIs.
 
   * Change \<SATProject\>/notebooks/Utils/initialize value of from  "use_mastercreds":"true" to "use_mastercreds":"false"
   * Run the \<SATProject\>/notebooks/Setup/2.generate_secrets_setup_file notebook.  Setup your PAT tokens for each of the workspaces under the "master_name_scope” 

    <img src="./images/setup_secrets.png" width="70%" height="70%">

    We generated a template file: \<SATProject\>/configs/setupsecrets.sh to make this easy for you with 
    [curl](https://docs.databricks.com/dev-tools/api/latest/authentication.html#store-tokens-in-a-netrc-file-and-use-them-in-curl), 
    copy and paste and run the commands from the file with your PAT token values. 
    You will need to [setup .netrc file](https://docs.databricks.com/dev-tools/api/latest/authentication.html#store-tokens-in-a-netrc-file-and-use-them-in-curl) to use this method

   Example:

    curl --netrc --request POST 'https://oregon.cloud.databricks.com/api/2.0/secrets/put' -d '{"scope":"
 sat_scope", "key":"sat_token_1657683783405197", "string_value":"<dapi...>"}' 
   
   
3. Test API Connections    
   * Test connections from your workspace to accounts API calls and all workspace API calls by running \<SATProject\>/notebooks/Setup/3. test_connections. The workspaces that didn't pass the connection test are marked in workspace_configs.csv with connection_test as False and are not analyzed.

    <img src="./images/test_connections.png" width="70%" height="70%">
   
4. Enable workspaces for SAT analysis
   * Enable workspaces by running \<SATProject\>/notebooks/Setup/4. enable_workspaces_for_sat.  This makes the registered workspaces ready for SAT to monitor 

    <img src="./images/enable_workspaces.png" width="70%" height="70%">
   
5. Import SAT dashboard template
   * We built a ready to go DBSQL dashboard for SAT. Import the dashboard by running \<SATProject\>/notebooks/Setup/5. import_dashboard_template

    <img src="./images/import_dashboard.png" width="70%" height="70%">   
   
6. Configure Alerts 
   SAT can deliver alerts via email via Databricks SQL Alerts. Import the alerts template by running \<SATProject\>/notebooks/Setup/6. configure_alerts_template (optional)

   <img src="./images/configure_alerts.png" width="70%" height="70%">
   
</details>
   
## Update  configuration files
            
1. Modify security_best_practices (Optional) 
   * Go to \<SATProject\>/notebooks/Setup/7. update_sat_check_configuration and use this utility to enable/disable a Check, modify Evaluation Value and Alert configuration value for each check. You can update this file any time and any analysis from there on will take these values into consideration. 
   * [Configure widget settings](https://docs.databricks.com/notebooks/widgets.html#configure-widget-settings-1) behavior "On Widget Change" for this notebooks to "Do Nothing"         
            
    <img src="./images/upate_security_best_practices.png" width="70%" height="70%">
 
2. Modify workspace_configs file (Required for manual checks values)
     * **Note**:  Limit number of workspaces to be analyzed by SAT to 100. 
     * **Tip**:  You can use this utility to turn on a specific workspace and turn off other workspaces for a specific run.
     * **Tip**:  You can use this utility to apply your edits to multiple workspaces settings by using "Apply Setting to all workspaces" option.
     
     * Go to\<SATProject\>/notebooks/Setup/8. update_workspace_configuration and  You will need to set analysis_enabled as True or False based on if you would like to enroll a workspace to analyze by the SAT.
      * [Configure widget settings](https://docs.databricks.com/notebooks/widgets.html#configure-widget-settings-1) behavior "On Widget Change" for this notebooks to "Do Nothing"   
            
     Set alert_subscriber_user_id to a valid user login email address to receive alerts by workspace
 
     Note: Please avoid  “+” character in the alert_subscriber_user_id values due to a limitation with the alerts API. 
     
     Update values for each workspace for the manual checks:(    sso_enabled,scim_enabled,vpc_peering_done,object_storage_encypted,table_access_control_enabled)
 
     * sso_enabled : True if you enabled Single Singn-on for the workspace
     * scim_enabled: True if you integrated with  SCIM for the workspace
     * vpc_peering_done: False if you have not peered with another VPC 
     * object_storage_encypted: True if you encrypted your data buckets
     * table_access_control_enabled : True if you enabled ACLs so that you can utilize Table ACL clusters that enforce user isolation  
     <img src="./images/update_workspace_configuration.png" width="70%" height="70%">  
 
## Usage
 
 (Estimated time to complete these steps: 5 - 10 mins per workspace)  
 **Note**:  Limit number of workspaces to be analyzed by SAT to 100. 
1. Attach and run the notebook \<SATProject\>/notebooks/security_analysis_driver 
   Note: This process takes upto 10 mins per workspace
 
   <img src="./images/run_analysis.png" width="70%" height="70%">
   
 
   At this point you should see **SAT** database and tables in your SQL Warehouses:

   <img src="./images/sat_database.png" width="70%" height="70%">
   
   
   
2. Access Databricks SQL Dashboards section and find "SAT - Security Analysis Tool" dashboard  to see the report. You can filter the dashboard by **SAT** tag. 
   
   <img src="./images/sat_dashboard_loc.png" width="70%" height="70%">

    **Note:** You need to select the workspace and date and click "Apply Changes" to get the report.  
    **Note:** The dashbord shows last valid run for the selected date if there is one, if not it shows the latest report for that workspace.  
 
    You can share SAT dashboard with other members of your team by using the "Share" functionality on the top right corner of the dashboard. 
      
     
    Here is what your SAT Dashboard should look like:
 
   <img src="../images/sat_dashboard_partial.png" width="50%" height="50%">   
    
3.  Activate Alerts 
  * Goto Alerts and find the alert(s) created by SAT tag and adjust the schedule to your needs. 


      <img src="./images/alerts_1.png" width="50%" height="50%">   
 

      <img src="./images/alerts_2.png" width="50%" height="50%">   

   
   
## Configure Workflow (Optional) 
 
 (Estimated time to complete these steps: 5 mins)  
 
  * Databricks Workflows is the fully-managed orchestration service. You can configure SAT to automate when and how you would like to schedule it by using by taking advantage of Workflows. 

  * Goto Workflows - > click on create jobs -> setup as following:

    Task Name  : security_analysis_driver

    Type: Notebook

    Source: Workspace (or your git clone of SAT)

    Path : \<SATProject\>/SAT/SecurityAnalysisTool-BranchV2Root/notebooks/security_analysis_driver

    Cluster: Make sure to pick the Single user mode job compute cluster you created before. 

    <img src="./images/workflow.png" width="50%" height="50%">   

    Add a schedule as per your needs. That’s it. Now you are continuously monitoring the health of your account workspaces.


## FAQs
 1. How can SAT be configured if access to github is not possible due to firewall restrictions to git or other organization policies?
    
    You can still setup SAT by downloading the [release zip](https://github.com/databricks-industry-solutions/security-analysis-tool/releases) file and by using Git repo to load SAT project into your workspace.
     * Add Repo by going to Repos in your workspace:  
     
      <img src="./images/add_repo_1.png" width="50%" height="50%">   
 
     * Type SAT as your "Repository name" and uncheck "Create repo by cloning a Git repository"
 
      <img src="./images/add_repo_2.png" width="50%" height="50%">
 
     * Click on the pulldown menu and click on Import
      
      <img src="./images/add_repo_3.png" width="50%" height="50%">

     * Drag and drop the release zip file and click Import
 
      <img src="./images/add_repo_4.png" width="50%" height="50%">
      
      <img src="./images/add_repo_5.png" width="50%" height="50%"> 
    
    You should see the SAT project in your workspace. 
    
## Troubleshooting
   
1. Incorrectly configured secrets
    * Error:
   
      Secret does not exist with scope: sat_scope and key: sat_tokens

    * Resolution:
      Check if the tokens are configured with the correct names by listing and comparing with the configuration.
      databricks secrets list --scope sat_scope

2. Invalid access token
   
    * Error:
   
      Error 403 Invalid access token.

    * Resolution: 
   
      Check your PAT token configuration for  “workspace_pat_token” key 

3. Firewall blocking databricks accounts console

    * Error: 
         <p/>   
         Traceback (most recent call last): File "/databricks/python/lib/python3.8/site-packages/urllib3/connectionpool.py", line 670, in urlopen  httplib_response = self._make_request(  File "/databricks/python/lib/python3.8/site-packages/urllib3/connectionpool.py", line 381, in _make_request  self._validate_conn(conn)  File "/databricks/python/lib/python3.8/site-packages/urllib3/connectionpool.py", line 978, in _validate_conn  conn.connect()  File "/databricks/python/lib/python3.8/site-packages/urllib3/connection.py", line 362, in connect  self.sock = ssl_wrap_socket(  File "/databricks/python/lib/python3.8/site-packages/urllib3/util/ssl_.py", line 386, in ssl_wrap_socket  return context.wrap_socket(sock, server_hostname=server_hostname)  File "/usr/lib/python3.8/ssl.py", line 500, in wrap_socket  return self.sslsocket_class._create(  File "/usr/lib/python3.8/ssl.py", line 1040, in _create  self.do_handshake()  File "/usr/lib/python3.8/ssl.py", line 1309, in do_handshake  self._sslobj.do_handshake() ConnectionResetError: [Errno 104] Connection reset by peer During handling of the above exception, another exception occurred:

    * Resolution: 
   
      Run this following command in your notebook %sh 
      curl -X GET -H "Authorization: Basic /<base64 of userid:password/>" -H "Content-Type: application/json" https://accounts.cloud.databricks.com/api/2.0/accounts/<account_id>/workspaces

            or
            
     %sh curl -u 'user:password' -X GET  “Content-Type: application/json” https://accounts.cloud.databricks.com/api/2.0/accounts/119f3ee2-8c38-4cdb-88e1-81c091c378a2/workspaces       
            
      If you don’t see a JSON with a clean listing of workspaces you are likely having a firewall issue that is blocking calls to the accounts console.  Please have your infrastructure team add Databricks accounts.cloud.databricks.com to the allow-list.   

4. Offline install of libraries incase of no PyPI access 

      Download the dbl_sat_sdk version specified in the notebook notebooks/utils/initialize from PyPi
      https://pypi.org/project/dbl-sat-sdk/
      Upload the dbl_sat_sdk-w.x.y-py3-none-any.whl to a dbfs location. You can use the databricks-cli as one mechanism to upload.
      for e.g.

      ```
      databricks --profile e2-satfs cp /localdrive/whlfile/dbl_sat_sdk-w.x.y-py3-none-any.whl dbfs:/FileStore/wheels/
      ```
      Additionally download the following wheel files and upload it to the dbfs location as above.
      https://pypi.org/project/requests/
            
      https://pypi.org/project/msal/
            
      https://pypi.org/project/google-auth/
            
      Update the notebook, notebooks/utils/initialize with the following:
            
      %pip install requests --find-links /dbfs/FileStore/wheels/requests.whl
            
      %pip install msal --find-links /dbfs/FileStore/wheels/msal.whl
            
      %pip install google-auth --find-links /dbfs/FileStore/wheels/google-auth.whl 
            
      %pip install dbl-sat-sdk=={SDK_VERSION} --find-links /dbfs/FileStore/wheels/dbl_sat_sdk-w.x.y-py3-none-any.whl

