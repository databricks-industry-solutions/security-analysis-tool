# Databricks notebook source
# MAGIC %md
# MAGIC **Functionality:** Analyzes all configured workspaces and generates a report of hardcoded secrets.

# COMMAND ----------

# MAGIC %run ./diagnosis/pre_run_config_check

# COMMAND ----------

# MAGIC %run ./Includes/install_sat_sdk

# COMMAND ----------

# MAGIC %run ./Utils/initialize

# COMMAND ----------

# MAGIC %run ./Utils/common

# COMMAND ----------

hostname = (
    dbutils.notebook.entry_point.getDbutils()
    .notebook()
    .getContext()
    .apiUrl()
    .getOrElse(None)
)
cloud_type = getCloudType(hostname)
clusterid = spark.conf.get("spark.databricks.clusterUsageTags.clusterId")
json_.update(
    {
        "url": hostname,
        "workspace_id": "accounts",
        "cloud_type": cloud_type,
        "clusterid": clusterid,
    }
)

# COMMAND ----------

dfexist = getWorkspaceConfig()
dfexist.filter(dfexist.analysis_enabled == True ).createOrReplaceTempView(
    "all_workspaces"
)

# COMMAND ----------

import json
from dbruntime.databricks_repl_context import get_context
current_workspace = get_context().workspaceId

# COMMAND ----------

# MAGIC %md
# MAGIC ##### The analysis executes across all Databricks workspaces configured for SAT. Eligible workspaces are determined based on entries in **`workspace_configs.csv`** and the **`security_analysis.account_workspaces`** table where the **`analysis_enabled`** flag is set to **`True`**.
# MAGIC ##### If a workspace does not appear in the results, verify that it is correctly listed and that **`analysis_enabled = True`** in both configuration sources.
# MAGIC ##### When running the job/notebook using **Serverless Compute**, the analysis is limited to the **current workspace**. To scan all eligible workspaces, use a Classic Compute cluster.

# COMMAND ----------

serverless_filter=""
if is_serverless:
    serverless_filter = " where workspace_id = '" + current_workspace + "'"

workspacesdf = spark.sql(f"select * from `all_workspaces` {serverless_filter}")
display(workspacesdf)
workspaces = workspacesdf.collect()
if workspaces is None or len(workspaces) == 0:
    loggr.info(
        "Workspaces are not configured for analysis, check the workspace_configs.csv and "
        + json_["analysis_schema_name"]
        + ".account_workspaces if analysis_enabled flag is enabled to True. Use security_analysis_initializer to auto configure workspaces for analysis. "
    )
    # dbutils.notebook.exit("Unsuccessful analysis.")

# COMMAND ----------

# MAGIC %md
# MAGIC ### TruffleHog Secret Scanning
# MAGIC ##### Runs TruffleHog secret scanning across all configured workspaces to detect exposed credentials and secrets in notebooks.

# COMMAND ----------

def generate_shared_run_id():
    """
    Generate a shared run_id for both notebook and cluster scans.
    This enables correlation between findings in the same scan run.

    Returns:
        int: New run ID (auto-generated by IDENTITY column)
    """
    try:
        # Insert new run into run_number_table (runID is auto-generated as IDENTITY column)
        spark.sql(f'''
            INSERT INTO {json_["analysis_schema_name"]}.run_number_table (check_time)
            VALUES (current_timestamp())
        ''')

        # Retrieve the auto-generated runID (it will be the max value)
        result = spark.sql(f'''
            SELECT max(runID) as new_run_id
            FROM {json_["analysis_schema_name"]}.run_number_table
        ''').collect()

        new_run_id = result[0]["new_run_id"]
        loggr.info(f"Generated shared run_id: {new_run_id}")
        return new_run_id

    except Exception as e:
        loggr.error(f"Failed to generate shared run_id: {str(e)}")
        # Fallback: use current timestamp as unique run ID
        import time
        fallback_run_id = int(time.time())
        loggr.info(f"Using fallback run_id: {fallback_run_id}")
        return fallback_run_id


def processTruffleHogScan(wsrow, run_id):
    """
    Process TruffleHog secret scanning for a single workspace.
    Similar to processWorkspace but focused on secrets detection.

    Args:
        wsrow: Workspace row with configuration
        run_id: Shared run ID for this scan run
    """
    import json

    hostname = "https://" + wsrow.deployment_url
    cloud_type = getCloudType(hostname)
    workspace_id = wsrow.workspace_id
    sso = wsrow.sso_enabled
    scim = wsrow.scim_enabled
    vpc_peering_done = wsrow.vpc_peering_done
    object_storage_encrypted = wsrow.object_storage_encrypted
    table_access_control_enabled = wsrow.table_access_control_enabled

    clusterid = spark.conf.get("spark.databricks.clusterUsageTags.clusterId")
    ws_json = dict(json_)
    ws_json.update(
        {
            "sso": sso,
            "scim": scim,
            "object_storage_encryption": object_storage_encrypted,
            "vpc_peering": vpc_peering_done,
            "table_access_control_enabled": table_access_control_enabled,
            "url": hostname,
            "workspace_id": workspace_id,
            "cloud_type": cloud_type,
            "clusterid": clusterid,
            "run_id": run_id,  # Add shared run_id
        }
    )

    loggr.info(f"Starting notebook scan for workspace: {workspace_id} (run_id: {run_id})")
    loggr.info(ws_json)

    # Run TruffleHog secret scanning for notebooks
    loggr.info(f"Running TruffleHog notebook scan for workspace: {workspace_id}")
    scan_result = dbutils.notebook.run(
        f"{basePath()}/notebooks/Includes/scan_secrets/notebook_secret_scan",
        3600,  # 1 hour timeout for secrets scanning
        {"json_": json.dumps(ws_json)},
    )
    loggr.info(f"Notebook scan completed for workspace: {workspace_id}")
    return scan_result


def processClusterScan(wsrow, run_id):
    """
    Process cluster configuration secret scanning for a single workspace.

    Args:
        wsrow: Workspace row with configuration
        run_id: Shared run ID for this scan run (same as notebook scan)

    Returns:
        str: JSON result from cluster scan
    """
    import json

    hostname = "https://" + wsrow.deployment_url
    cloud_type = getCloudType(hostname)
    workspace_id = wsrow.workspace_id

    clusterid = spark.conf.get("spark.databricks.clusterUsageTags.clusterId")
    ws_json = dict(json_)
    ws_json.update(
        {
            "url": hostname,
            "workspace_id": workspace_id,
            "cloud_type": cloud_type,
            "clusterid": clusterid,
            "run_id": run_id,  # Add shared run_id
        }
    )

    loggr.info(f"Starting cluster scan for workspace: {workspace_id} (run_id: {run_id})")

    # Run cluster configuration secret scanning
    loggr.info(f"Running cluster config scan for workspace: {workspace_id}")
    scan_result = dbutils.notebook.run(
        f"{basePath()}/notebooks/Includes/scan_secrets/cluster_secrets_scan",
        3600,  # 1 hour timeout for cluster scanning
        {"json_": json.dumps(ws_json)},
    )
    loggr.info(f"Cluster scan completed for workspace: {workspace_id}")
    return scan_result


def runTruffleHogScanForAllWorkspaces():
    """
    Run secret scanning (notebooks + clusters) for all configured workspaces.
    Each workspace gets a shared run_id for correlation between notebook and cluster findings.
    """
    loggr.info("Starting secret scanning for all configured workspaces")

    scan_workspaces = workspaces

    if scan_workspaces is None or len(scan_workspaces) == 0:
        loggr.info("No workspaces configured for secret scanning")
        return

    loggr.info(f"Running secret scans on {len(scan_workspaces)} workspace(s)")

    # Run secret scanning (sequential for now to avoid overwhelming the system)
    for ws in scan_workspaces:
        try:
            # Generate shared run_id for this workspace
            shared_run_id = generate_shared_run_id()
            loggr.info(f"Starting scans for workspace: {ws.workspace_id} (run_id: {shared_run_id})")
            print(f"üîç Starting scans for workspace: {ws.workspace_id} (run_id: {shared_run_id})")

            # Scan notebooks
            try:
                loggr.info(f"Starting notebook scan for workspace: {ws.workspace_id}")
                notebook_result = processTruffleHogScan(ws, shared_run_id)
                loggr.info(f"Notebook scan completed for workspace: {ws.workspace_id}")
                print(f"  ‚úÖ Notebook scan completed")
            except Exception as e:
                loggr.error(f"Notebook scan failed for workspace {ws.workspace_id}: {str(e)}")
                print(f"  ‚ùå Notebook scan failed: {str(e)}")
                # Continue to cluster scan even if notebook scan fails

            # Scan clusters
            try:
                loggr.info(f"Starting cluster scan for workspace: {ws.workspace_id}")
                cluster_result = processClusterScan(ws, shared_run_id)
                loggr.info(f"Cluster scan completed for workspace: {ws.workspace_id}")
                print(f"  ‚úÖ Cluster scan completed")
            except Exception as e:
                loggr.error(f"Cluster scan failed for workspace {ws.workspace_id}: {str(e)}")
                print(f"  ‚ùå Cluster scan failed: {str(e)}")
                # Continue to next workspace

            print(f"‚úÖ All scans completed for workspace: {ws.workspace_id}")

        except Exception as e:
            loggr.error(f"Fatal error scanning workspace {ws.workspace_id}: {str(e)}")
            print(f"‚ùå Fatal error for workspace: {ws.workspace_id}")
            continue


runTruffleHogScanForAllWorkspaces()
